{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_hash(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "def check_duplicates(set1, set2):\n",
    "    hashes = {}\n",
    "    duplicates = []\n",
    "\n",
    "    # Process all files in both sets and store their hashes\n",
    "    for dataset_path in [set1, set2]:\n",
    "        for root, _, files in os.walk(dataset_path):\n",
    "            for filename in files:\n",
    "                if filename.endswith('jpg'):  # Add other file types if needed\n",
    "                    file_path = os.path.join(root, filename)\n",
    "                    filehash = file_hash(file_path)\n",
    "                    if filehash in hashes:\n",
    "                        duplicates.append((hashes[filehash], file_path))\n",
    "                    else:\n",
    "                        hashes[filehash] = file_path\n",
    "    return duplicates\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = check_duplicates('raw_data/Training', 'raw_data/Testing')\n",
    "if duplicates:\n",
    "    print(\"Duplicates found:\", len(duplicates))\n",
    "    # for dup in duplicates:\n",
    "    #     print(f\"Duplicate: {dup[0]} and {dup[1]}\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'notumor': 0, 'glioma': 1, 'meningioma': 2, 'pituitary': 3}\n",
    "image_size = 150\n",
    "\n",
    "def preprocess_data(image, image_size):\n",
    "    image_np = np.array(image)\n",
    "    image_np = cv2.bilateralFilter(image_np, 2, 50, 50)\n",
    "    image_np = cv2.resize(image_np, (image_size, image_size))\n",
    "    return Image.fromarray(image_np)\n",
    "\n",
    "def load_unique_images(base_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    hashes = set()\n",
    "    \n",
    "    for partition in ('Training', 'Testing'):\n",
    "        for label in label_map.keys():\n",
    "            path = os.path.join(base_path, partition, label)\n",
    "            for file in tqdm(os.listdir(path)):\n",
    "                file_path = os.path.join(path, file)\n",
    "\n",
    "                img_hash = file_hash(file_path)\n",
    "                if img_hash in hashes:\n",
    "                    continue\n",
    "                hashes.add(img_hash)\n",
    "\n",
    "                # process image\n",
    "                image = Image.open(file_path).convert('L')\n",
    "                image = preprocess_data(image, image_size)\n",
    "                images.append(image)\n",
    "                labels.append(label_map[label])\n",
    "    \n",
    "    return (images, labels)\n",
    "\n",
    "all_images, all_labels = load_unique_images('raw_data')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process training sets (no data augment and data augment)\n",
    "base_image_train = deepcopy(x_train)\n",
    "rotate_image_train = deepcopy(x_train)\n",
    "\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.ToTensor() # already normalizes the image\n",
    "])\n",
    "base_image_train = [simple_transform(image) for image in base_image_train]\n",
    "\n",
    "rotate_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(180), \n",
    "    transforms.ToTensor() \n",
    "])\n",
    "rotate_image_train = [rotate_transform(image) for image in rotate_image_train]\n",
    "\n",
    "# test on fixed rotated images\n",
    "test_images = []\n",
    "test_labels = []\n",
    "test_rotate_info = []\n",
    "rotation_degrees = [theta for theta in range(0, 360, 22.5)]\n",
    "for image, label in zip(x_test, y_test):\n",
    "    for angle in rotation_degrees:\n",
    "        rotated_image = image.copy()\n",
    "        rotated_image = rotated_image.rotate(angle)\n",
    "        rotated_image = simple_transform(rotated_image) # converts to Tensor\n",
    "        test_images.append(rotated_image)\n",
    "        test_labels.append(label)\n",
    "        test_rotate_info.append(angle)\n",
    "\n",
    "# convert to TensorDatasets and DataLoaders\n",
    "base_image_train = torch.stack(base_image_train)\n",
    "rotate_image_train = torch.stack(rotate_image_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "test_images = torch.stack(test_images)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_rotate_info = torch.tensor(test_rotate_info)\n",
    "\n",
    "base_train_loader = DataLoader(TensorDataset(base_image_train, y_train), batch_size=32,\n",
    "                               shuffle=True,\n",
    "                               pin_memory=True,\n",
    "                               num_workers=3)\n",
    "rotated_train_loader = DataLoader(TensorDataset(rotate_image_train, y_train), batch_size=32,\n",
    "                               shuffle=True,\n",
    "                               pin_memory=True,\n",
    "                               num_workers=3)\n",
    "test_loader = DataLoader(TensorDataset(test_images, test_labels, test_rotate_info), batch_size=32,\n",
    "                         shuffle=False,\n",
    "                         pin_memory=True,\n",
    "                         num_workers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StandardCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 75 * 75, 128)  # Adjust if different size or pooling\n",
    "        self.fc2 = nn.Linear(128, 4)  # Output layer for multiclass classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)  # Pooling to reduce spatial dimensions\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No activation, raw logits for CrossEntropyLoss\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RIC_CNN(nn.Module): # TODO\n",
    "    def __init__(self):\n",
    "        super(RIC_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 75 * 75, 128)  # Adjust if different size or pooling\n",
    "        self.fc2 = nn.Linear(128, 4)  # Output layer for multiclass classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)  # Pooling to reduce spatial dimensions\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No activation, raw logits for CrossEntropyLoss\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiclass_model(model, optimizer, criterion, training_loader, evaluate_loader, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in tqdm(training_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "        evaluate_multiclass_model(model, evaluate_loader)\n",
    "\n",
    "def evaluate_multiclass_model(model, evaluate_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in evaluate_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Standard, No Augment\")\n",
    "no_augment_model = StandardCNN().to(device)\n",
    "no_augment_criterion = nn.CrossEntropyLoss()  # Suitable for multiclass\n",
    "no_augment_optimizer = torch.optim.Adam(no_augment_model.parameters(), lr=0.001)\n",
    "train_multiclass_model(no_augment_model, no_augment_optimizer, no_augment_criterion, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Standard, No Augment\")\n",
    "augment_model = StandardCNN().to(device)\n",
    "augment_criterion = nn.CrossEntropyLoss()  # Suitable for multiclass\n",
    "augment_optimizer = torch.optim.Adam(augment_model.parameters(), lr=0.001)\n",
    "train_multiclass_model(augment_model, augment_optimizer, augment_criterion, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training RIC-CNN, No Augment\")\n",
    "riccnn_model = RIC_CNN().to(device)\n",
    "riccnn_criterion = nn.CrossEntropyLoss()  # Suitable for multiclass\n",
    "riccnn_optimizer = torch.optim.Adam(riccnn_model.parameters(), lr=0.001)\n",
    "train_multiclass_model(riccnn_model, riccnn_optimizer, riccnn_criterion, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
