{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 297\n"
     ]
    }
   ],
   "source": [
    "def file_hash(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "def check_duplicates(set1, set2):\n",
    "    hashes = {}\n",
    "    duplicates = []\n",
    "\n",
    "    # Process all files in both sets and store their hashes\n",
    "    for dataset_path in [set1, set2]:\n",
    "        for root, _, files in os.walk(dataset_path):\n",
    "            for filename in files:\n",
    "                if filename.endswith('jpg'):  # Add other file types if needed\n",
    "                    file_path = os.path.join(root, filename)\n",
    "                    filehash = file_hash(file_path)\n",
    "                    if filehash in hashes:\n",
    "                        duplicates.append((hashes[filehash], file_path))\n",
    "                    else:\n",
    "                        hashes[filehash] = file_path\n",
    "    return duplicates\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = check_duplicates('raw_data/Training', 'raw_data/Testing')\n",
    "if duplicates:\n",
    "    print(\"Duplicates found:\", len(duplicates))\n",
    "    # for dup in duplicates:\n",
    "    #     print(f\"Duplicate: {dup[0]} and {dup[1]}\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1595/1595 [00:01<00:00, 923.06it/s] \n",
      "100%|██████████| 1321/1321 [00:01<00:00, 798.11it/s]\n",
      "100%|██████████| 1339/1339 [00:02<00:00, 665.65it/s]\n",
      "100%|██████████| 1457/1457 [00:02<00:00, 614.88it/s]\n",
      "100%|██████████| 405/405 [00:00<00:00, 1169.75it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 658.32it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 707.79it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 680.71it/s]\n"
     ]
    }
   ],
   "source": [
    "label_map = {'notumor': 0, 'glioma': 1, 'meningioma': 2, 'pituitary': 3}\n",
    "image_size = 150\n",
    "\n",
    "def preprocess_data(image, image_size):\n",
    "    image_np = np.array(image)\n",
    "    image_np = cv2.bilateralFilter(image_np, 2, 50, 50)\n",
    "    image_np = cv2.resize(image_np, (image_size, image_size))\n",
    "    return Image.fromarray(image_np)\n",
    "\n",
    "def load_unique_images(base_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    hashes = set()\n",
    "    \n",
    "    for partition in ('Training', 'Testing'):\n",
    "        for label in label_map.keys():\n",
    "            path = os.path.join(base_path, partition, label)\n",
    "            for file in tqdm(os.listdir(path)):\n",
    "                file_path = os.path.join(path, file)\n",
    "\n",
    "                img_hash = file_hash(file_path)\n",
    "                if img_hash in hashes:\n",
    "                    continue\n",
    "                hashes.add(img_hash)\n",
    "\n",
    "                # process image\n",
    "                image = Image.open(file_path).convert('L')\n",
    "                image = preprocess_data(image, image_size)\n",
    "                images.append(image)\n",
    "                labels.append(label_map[label])\n",
    "    \n",
    "    return (images, labels)\n",
    "\n",
    "all_images, all_labels = load_unique_images('raw_data')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process training sets (no data augment and data augment)\n",
    "base_image_train = deepcopy(x_train)\n",
    "rotate_image_train = deepcopy(x_train)\n",
    "\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.ToTensor() # already normalizes the image\n",
    "])\n",
    "base_image_train = [simple_transform(image) for image in base_image_train]\n",
    "\n",
    "rotate_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(180), # TODO check if it's +/- 180 degrees\n",
    "    transforms.ToTensor() \n",
    "])\n",
    "rotate_image_train = [rotate_transform(image) for image in rotate_image_train]\n",
    "\n",
    "# test on fixed rotated images\n",
    "test_images = []\n",
    "test_labels = []\n",
    "test_rotate_info = []\n",
    "rotation_degrees = [theta for theta in range(0, 360, 45)] # TODO fix.\n",
    "for image, label in zip(x_test, y_test):\n",
    "    for angle in rotation_degrees:\n",
    "        rotated_image = image.copy()\n",
    "        rotated_image = rotated_image.rotate(angle)\n",
    "        rotated_image = simple_transform(rotated_image) # converts to Tensor\n",
    "        test_images.append(rotated_image)\n",
    "        test_labels.append(label)\n",
    "        test_rotate_info.append(angle)\n",
    "\n",
    "# convert to TensorDatasets and DataLoaders\n",
    "base_image_train = torch.stack(base_image_train)\n",
    "rotate_image_train = torch.stack(rotate_image_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "test_images = torch.stack(test_images)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_rotate_info = torch.tensor(test_rotate_info)\n",
    "\n",
    "base_train_loader = DataLoader(TensorDataset(base_image_train, y_train), batch_size=32,\n",
    "                               shuffle=True,\n",
    "                               pin_memory=True,\n",
    "                               num_workers=3)\n",
    "rotated_train_loader = DataLoader(TensorDataset(rotate_image_train, y_train), batch_size=32,\n",
    "                               shuffle=True,\n",
    "                               pin_memory=True,\n",
    "                               num_workers=3)\n",
    "test_loader = DataLoader(TensorDataset(test_images, test_labels, test_rotate_info), batch_size=32,\n",
    "                         shuffle=False,\n",
    "                         pin_memory=True,\n",
    "                         num_workers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StandardCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 75 * 75, 128)  # Adjust if different size or pooling\n",
    "        self.fc2 = nn.Linear(128, 4)  # Output layer for multiclass classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)  # Pooling to reduce spatial dimensions\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No activation, raw logits for CrossEntropyLoss\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RIC_CNN(nn.Module): # TODO\n",
    "    def __init__(self):\n",
    "        super(RIC_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 75 * 75, 128)  # Adjust if different size or pooling\n",
    "        self.fc2 = nn.Linear(128, 4)  # Output layer for multiclass classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)  # Pooling to reduce spatial dimensions\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No activation, raw logits for CrossEntropyLoss\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiclass_model(model, optimizer, criterion, training_loader, evaluate_loader, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in tqdm(training_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "        evaluate_multiclass_model(model, evaluate_loader)\n",
    "\n",
    "def evaluate_multiclass_model(model, evaluate_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, angle in evaluate_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Standard, No Augment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:26<00:00,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.1576677560806274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 58.31166419019316%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.018324390053749084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 59.39821693907875%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.0012924617622047663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 58.68313521545319%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.3718293011188507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 55.75780089153046%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.003880410920828581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 58.228083209509656%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.0002983130398206413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 56.61218424962853%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.000741605123039335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 57.40156017830609%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:17<00:00,  9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.006001129746437073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 56.705052005943536%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.25537776947021484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 55.72994056463596%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.005372744053602219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 58.16307578008915%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Standard, No Augment\")\n",
    "no_augment_model = StandardCNN().to(device)\n",
    "no_augment_criterion = nn.CrossEntropyLoss()  # Suitable for multiclass\n",
    "no_augment_optimizer = torch.optim.Adam(no_augment_model.parameters(), lr=0.001)\n",
    "train_multiclass_model(no_augment_model, no_augment_optimizer, no_augment_criterion,\n",
    "                       base_train_loader, test_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Standard, No Augment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:18<00:00,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7037447690963745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 70.30089153046062%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.5434268116950989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 73.7555720653789%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.04845966771245003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.55089153046062%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.548564076423645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.94093610698366%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.01842021383345127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.99034175334324%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.006633531767874956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.76448736998515%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.0008136564283631742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.36515601783061%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.0012426883913576603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.78603268945022%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.0002514365769457072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.70245170876672%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.0008738641627132893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 74.89784546805349%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Standard, No Augment\")\n",
    "augment_model = StandardCNN().to(device)\n",
    "augment_criterion = nn.CrossEntropyLoss()  # Suitable for multiclass\n",
    "augment_optimizer = torch.optim.Adam(augment_model.parameters(), lr=0.001)\n",
    "train_multiclass_model(augment_model, augment_optimizer, augment_criterion,\n",
    "                       rotated_train_loader, test_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RIC-CNN, No Augment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [01:45<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.10363948345184326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 58.62741456166419%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [02:19<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.184637188911438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 56.59361069836553%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 8/169 [00:07<02:29,  1.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m riccnn_criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()  \u001b[38;5;66;03m# Suitable for multiclass\u001b[39;00m\n\u001b[1;32m      4\u001b[0m riccnn_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(riccnn_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_multiclass_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mriccnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mriccnn_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mriccnn_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mbase_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mtrain_multiclass_model\u001b[0;34m(model, optimizer, criterion, training_loader, evaluate_loader, num_epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(training_loader):\n\u001b[0;32m----> 6\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training RIC-CNN, No Augment\")\n",
    "riccnn_model = RIC_CNN().to(device)\n",
    "riccnn_criterion = nn.CrossEntropyLoss()  # Suitable for multiclass\n",
    "riccnn_optimizer = torch.optim.Adam(riccnn_model.parameters(), lr=0.001)\n",
    "train_multiclass_model(riccnn_model, riccnn_optimizer, riccnn_criterion, \n",
    "                       base_train_loader, test_loader, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
