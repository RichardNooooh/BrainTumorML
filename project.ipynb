{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import imutils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "x_train = [] # training images.\n",
    "y_train  = [] # training labels.\n",
    "x_test = [] # testing images.\n",
    "y_test = [] # testing labels.\n",
    "\n",
    "image_size = 150\n",
    "\n",
    "for label in labels:\n",
    "    trainPath = os.path.join('raw_data/Training',label)\n",
    "    for file in tqdm(os.listdir(trainPath)):\n",
    "        image = cv2.imread(os.path.join(trainPath, file), 0) # load images in gray.\n",
    "        image = cv2.bilateralFilter(image, 2, 50, 50) # remove images noise.\n",
    "        #image = cv2.applyColorMap(image, cv2.COLORMAP_BONE) # produce a pseudocolored image.\n",
    "        image = cv2.resize(image, (image_size, image_size)) # resize images into 200*200.\n",
    "        x_train.append(image)\n",
    "        y_train.append(1 if label != 'notumor' else 0)\n",
    "    \n",
    "    testPath = os.path.join('raw_data/Testing',label)\n",
    "    for file in tqdm(os.listdir(testPath)):\n",
    "        image = cv2.imread(os.path.join(testPath, file), 0)\n",
    "        image = cv2.bilateralFilter(image, 2, 50, 50)\n",
    "        #image = cv2.applyColorMap(image, cv2.COLORMAP_BONE)\n",
    "        image = cv2.resize(image, (image_size, image_size))\n",
    "        x_test.append(image)\n",
    "        y_test.append(1 if label != 'notumor' else 0)\n",
    "\n",
    "\n",
    "x_train = np.array(x_train) / 255.0 # normalize Images into range 0 to 1.\n",
    "x_test = np.array(x_test) / 255.0\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((150, 150)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485], std=[0.229])  # Adjust these values based on your dataset\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "train_images_tensor = torch.tensor(x_train).float() / 255.0  # Normalize if not already done\n",
    "train_labels_tensor = torch.tensor(np.array(y_train)).long()  # Ensure labels are in long format for CE Loss\n",
    "\n",
    "test_images_tensor = torch.tensor(x_test).float() / 255.0\n",
    "test_labels_tensor = torch.tensor(np.array(y_test)).long()\n",
    "\n",
    "# Reshape the tensors to add a channel dimension (1, 150, 150)\n",
    "train_images_tensor = train_images_tensor.unsqueeze(1)\n",
    "test_images_tensor = test_images_tensor.unsqueeze(1)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainMRINet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BrainMRINet, self).__init__()\n",
    "        # in_channels = the number of channels of the input to the convolutional layer (greyscale = 1, rgb = 3)\n",
    "        # out_channels = the number of feature maps \n",
    "        # padding = (kernel_size-1) / 2\n",
    "        # Use Conv2D since our image data is 2D.\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # Change input channels if your images are not grayscale\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 75 * 75, 128)  # Adjust the size here based on the output of your last pooling layer\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = BrainMRINet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "        evaluate_model()\n",
    "\n",
    "def evaluate_model():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy}%')\n",
    "\n",
    "\n",
    "train_model(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
