{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import imutils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1321/1321 [00:02<00:00, 630.43it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 672.18it/s]\n",
      "100%|██████████| 1339/1339 [00:02<00:00, 626.80it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 693.87it/s]\n",
      "100%|██████████| 1595/1595 [00:01<00:00, 996.80it/s] \n",
      "100%|██████████| 405/405 [00:00<00:00, 1347.24it/s]\n",
      "100%|██████████| 1457/1457 [00:02<00:00, 628.20it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 404.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5712, 150, 150)\n",
      "(1311, 150, 150)\n"
     ]
    }
   ],
   "source": [
    "labels = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "x_train = [] # training images.\n",
    "y_train  = [] # training labels.\n",
    "x_test = [] # testing images.\n",
    "y_test = [] # testing labels.\n",
    "\n",
    "image_size = 150\n",
    "\n",
    "for label in labels:\n",
    "    trainPath = os.path.join('raw_data/Training',label)\n",
    "    for file in tqdm(os.listdir(trainPath)):\n",
    "        image = cv2.imread(os.path.join(trainPath, file), 0) # load images in gray.\n",
    "        image = cv2.bilateralFilter(image, 2, 50, 50) # remove images noise.\n",
    "        #image = cv2.applyColorMap(image, cv2.COLORMAP_BONE) # produce a pseudocolored image.\n",
    "        image = cv2.resize(image, (image_size, image_size)) # resize images into 200*200.\n",
    "        x_train.append(image)\n",
    "        y_train.append(1 if label != 'notumor' else 0)\n",
    "    \n",
    "    testPath = os.path.join('raw_data/Testing',label)\n",
    "    for file in tqdm(os.listdir(testPath)):\n",
    "        image = cv2.imread(os.path.join(testPath, file), 0)\n",
    "        image = cv2.bilateralFilter(image, 2, 50, 50)\n",
    "        #image = cv2.applyColorMap(image, cv2.COLORMAP_BONE)\n",
    "        image = cv2.resize(image, (image_size, image_size))\n",
    "        x_test.append(image)\n",
    "        y_test.append(1 if label != 'notumor' else 0)\n",
    "\n",
    "\n",
    "x_train = np.array(x_train) / 255.0 # normalize Images into range 0 to 1.\n",
    "x_test = np.array(x_test) / 255.0\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((150, 150)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485], std=[0.229])  # Adjust these values based on your dataset\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "train_images_tensor = torch.tensor(x_train).float() / 255.0  # Normalize if not already done\n",
    "train_labels_tensor = torch.tensor(np.array(y_train)).long()  # Ensure labels are in long format for CE Loss\n",
    "\n",
    "test_images_tensor = torch.tensor(x_test).float() / 255.0\n",
    "test_labels_tensor = torch.tensor(np.array(y_test)).long()\n",
    "\n",
    "# Reshape the tensors to add a channel dimension (1, 150, 150)\n",
    "train_images_tensor = train_images_tensor.unsqueeze(1)\n",
    "test_images_tensor = test_images_tensor.unsqueeze(1)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=3)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainMRINet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BrainMRINet, self).__init__()\n",
    "        # in_channels = the number of channels of the input to the convolutional layer (greyscale = 1, rgb = 3)\n",
    "        # out_channels = the number of feature maps \n",
    "        # padding = (kernel_size-1) / 2\n",
    "        # Use Conv2D since our image data is 2D.\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # Change input channels if your images are not grayscale\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 75 * 75, 128)  # Adjust the size here based on the output of your last pooling layer\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = BrainMRINet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:19<00:00,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.01213163137435913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1977.116704805492%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:16<00:00,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Loss: 0.36424708366394043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1977.116704805492%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 76/90 [00:14<00:02,  5.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# evaluate_model()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m----> 6\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "        evaluate_model()\n",
    "\n",
    "def evaluate_model():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy}%\\n')\n",
    "\n",
    "\n",
    "train_model(50)\n",
    "# evaluate_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
