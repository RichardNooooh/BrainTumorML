{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "# import imutils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import hashlib\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 297\n"
     ]
    }
   ],
   "source": [
    "def file_hash(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "def check_duplicates(set1, set2):\n",
    "    hashes = {}\n",
    "    duplicates = []\n",
    "\n",
    "    # Process all files in both sets and store their hashes\n",
    "    for dataset_path in [set1, set2]:\n",
    "        for root, _, files in os.walk(dataset_path):\n",
    "            for filename in files:\n",
    "                if filename.endswith('jpg'):  # Add other file types if needed\n",
    "                    file_path = os.path.join(root, filename)\n",
    "                    filehash = file_hash(file_path)\n",
    "                    if filehash in hashes:\n",
    "                        duplicates.append((hashes[filehash], file_path))\n",
    "                    else:\n",
    "                        hashes[filehash] = file_path\n",
    "    return duplicates\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = check_duplicates('raw_data/Training', 'raw_data/Testing')\n",
    "if duplicates:\n",
    "    print(\"Duplicates found:\", len(duplicates))\n",
    "    # for dup in duplicates:\n",
    "    #     print(f\"Duplicate: {dup[0]} and {dup[1]}\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is... really bad. There are 297 duplicates, and some of those duplicates are in the testing AND training sets. No wonder some of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1595/1595 [00:02<00:00, 771.91it/s]\n",
      "100%|██████████| 1321/1321 [00:02<00:00, 612.72it/s]\n",
      "100%|██████████| 1339/1339 [00:02<00:00, 585.49it/s]\n",
      "100%|██████████| 1457/1457 [00:02<00:00, 544.84it/s]\n",
      "100%|██████████| 405/405 [00:00<00:00, 1083.81it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 627.17it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 692.13it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 598.31it/s]\n"
     ]
    }
   ],
   "source": [
    "label_map = {'notumor': 0, 'glioma': 1, 'meningioma': 2, 'pituitary': 3}\n",
    "image_size = 150\n",
    "\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.ToTensor() # already normalizes the image\n",
    "])\n",
    "\n",
    "def preprocess_data(image, image_size):\n",
    "    image_np = np.array(image)\n",
    "    image_np = cv2.bilateralFilter(image_np, 2, 50, 50)\n",
    "    image_np = cv2.resize(image_np, (image_size, image_size))\n",
    "    return Image.fromarray(image_np)\n",
    "\n",
    "def load_unique_images(base_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    hashes = set()\n",
    "    \n",
    "    for partition in ('Training', 'Testing'):\n",
    "        for label in label_map.keys():\n",
    "            path = os.path.join(base_path, partition, label)\n",
    "            for file in tqdm(os.listdir(path)):\n",
    "                file_path = os.path.join(path, file)\n",
    "\n",
    "                img_hash = file_hash(file_path)\n",
    "                if img_hash in hashes:\n",
    "                    continue\n",
    "                hashes.add(img_hash)\n",
    "\n",
    "                # process image\n",
    "                image = Image.open(file_path).convert('L')\n",
    "                image = preprocess_data(image, image_size)\n",
    "                image = simple_transform(image)\n",
    "                images.append(image)\n",
    "                \n",
    "                # (binary label, multiclass label)\n",
    "                labels.append((label_map[label]))\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "all_images, all_labels = load_unique_images('raw_data')\n",
    "\n",
    "x_train, x_test, y_multi_train, y_multi_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution\n",
      "    Binary:\n",
      "        Training Counter({1: 4005, 0: 1375})\n",
      "        Test Counter({1: 990, 0: 356})\n",
      "    Multiclass:\n",
      "        Training Counter({3: 1408, 0: 1375, 2: 1309, 1: 1288})\n",
      "        Test Counter({0: 356, 3: 332, 1: 332, 2: 326})\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = torch.stack(x_train), torch.stack(x_test)\n",
    "\n",
    "y_binary_train = [int(label != 0) for label in y_multi_train]\n",
    "y_binary_test =  [int(label != 0) for label in y_multi_test]\n",
    "\n",
    "print(\"Label Distribution\")\n",
    "print(\"    Binary:\")\n",
    "print(\"        Training\", Counter(y_binary_train))\n",
    "print(\"        Test\", Counter(y_binary_test))\n",
    "print(\"    Multiclass:\")\n",
    "print(\"        Training\", Counter(y_multi_train))\n",
    "print(\"        Test\", Counter(y_multi_test))\n",
    "\n",
    "y_binary_train, y_binary_test = torch.tensor(y_binary_train), torch.tensor(y_binary_test)\n",
    "y_multi_train, y_multi_test = torch.tensor(y_multi_train), torch.tensor(y_multi_test)\n",
    "\n",
    "binary_training = DataLoader(TensorDataset(x_train, y_binary_train), batch_size=32,\n",
    "                             shuffle=True,\n",
    "                             pin_memory=True,\n",
    "                             num_workers=3)\n",
    "binary_testing = DataLoader(TensorDataset(x_test, y_binary_test), batch_size=32,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True,\n",
    "                             num_workers=3)\n",
    "multi_training = DataLoader(TensorDataset(x_train, y_multi_train), batch_size=32,\n",
    "                             shuffle=True,\n",
    "                             pin_memory=True,\n",
    "                             num_workers=3)\n",
    "multi_testing = DataLoader(TensorDataset(x_test, y_multi_test), batch_size=32,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True,\n",
    "                             num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        # in_channels = the number of channels of the input to the convolutional layer (greyscale = 1, rgb = 3)\n",
    "        # out_channels = the number of feature maps \n",
    "        # padding = (kernel_size-1) / 2\n",
    "        # Use Conv2D since our image data is 2D.\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # Change input channels if your images are not grayscale\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 75 * 75, 128)  # Adjust the size here based on the output of your last pooling layer\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # we are using BCEWithLogitsLoss\n",
    "        return x\n",
    "\n",
    "binary_model = BinaryClassification().to(device)\n",
    "binary_criterion = nn.BCEWithLogitsLoss()\n",
    "binary_optimizer = torch.optim.Adam(binary_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:18<00:00,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.004723594058305025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.2852897473997%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.015162359923124313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.84546805349183%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.0014109177282080054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.91976225854383%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 3.740075044333935e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.87964338781575%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 4.341838211985305e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.58841010401188%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 1.1920927533992653e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.58841010401188%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.58841010401188%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 2.3841794245527126e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.58841010401188%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 4.470344947549165e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.58841010401188%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.58841010401188%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_binary_model(num_epochs):\n",
    "    binary_model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in tqdm(binary_training):\n",
    "            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            binary_optimizer.zero_grad()\n",
    "            outputs = binary_model(images)\n",
    "            loss = binary_criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            binary_optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "        evaluate_binary_model()\n",
    "\n",
    "def evaluate_binary_model():\n",
    "    binary_model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in binary_testing:\n",
    "            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)\n",
    "            outputs = binary_model(images)\n",
    "            probs = torch.sigmoid(outputs)  # Convert logits to probabilities\n",
    "            predicted = probs.round()  # Convert probabilities to 0 or 1\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy}%\\n')\n",
    "\n",
    "\n",
    "train_binary_model(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 75 * 75, 128)  # Adjust if different size or pooling\n",
    "        self.fc2 = nn.Linear(128, 4)  # Output layer for multiclass classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)  # Pooling to reduce spatial dimensions\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No activation, raw logits for CrossEntropyLoss\n",
    "        return x\n",
    "\n",
    "# Example instantiation and setup\n",
    "multiclass_model = MulticlassClassification().to(device)\n",
    "multiclass_criterion = nn.CrossEntropyLoss()  # Suitable for multiclass\n",
    "multiclass_optimizer = torch.optim.Adam(multiclass_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:17<00:00,  9.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.8880287408828735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.8038632986627%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.9556090831756592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.9182763744428%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.2808113992214203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 89.67310549777118%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.00995013676583767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.09063893016345%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.06787999719381332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.08469539375929%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 3.0217732273740694e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.79346210995543%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.0008965736487880349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.38187221396731%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.43215009570121765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 87.66716196136701%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.0002233274863101542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.23922734026746%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:16<00:00, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 3.146986637148075e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.53640416047548%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_multiclass_model(model, optimizer, criterion, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in tqdm(multi_training):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "        evaluate_multiclass_model(model)\n",
    "\n",
    "def evaluate_multiclass_model(model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in multi_testing:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy}%\\n')\n",
    "\n",
    "# Example of training the model\n",
    "num_epochs = 10\n",
    "train_multiclass_model(multiclass_model, multiclass_optimizer, multiclass_criterion, num_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
